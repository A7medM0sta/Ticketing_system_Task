# -*- coding: utf-8 -*-
"""BertArabicMiniMeduim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZRzwdN7C--i8vFADslYDwpkPwQVLUGXb
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Description
our are tasked with implementing a text-based machine learning classifier. The dataset is included in the email attachment. Note that the data is unlabeled, and part of the assignment involves labeling the data using a technique
of your choice. Labels should be derived exclusively from the "Request Detail" or "Notes" fields. Ideally, the labels
should highlight discrepancies between the “Request Detail” and “Notes” features in Arabic, but English labels are
acceptable as well. Evaluation will be based on the range of coding techniques used, the professionalism of the results
presentation, and the model's performance.

## Technical Requirements:

### 1. Preprocessing
   1. Data cleaning (e.g., remove punctuation, stemming, stopwords, etc.), considering the emails thread and the bilingual nature of the content.
   2. Exploratory Data Analysis (EDA):
      - a. Document the 3-5 major insights from the data.
      - b. Show words most used in the “Request Detail” feature and present it visually in a clear and engaging way.
      - c. Study correlation between key variables.
      - d. Show distributions and dimensions of the data.
      - e. Display statistical measures, including tendency metrics.
   3. Apply dimensionality reduction methods (e.g., PCA, correlation analysis).
   4. Identify feature importance using algorithms.
   5. Feature engineering to improve model performance.
   6. Extract at least two columns of labels from features (e.g., "Request Detail" or "Notes") by identifying key terms that can serve as labels for the data. Example label types:
      - Classify tickets as "in scope" or "out of scope."
      - Classify tasks as "challenging" or "non-challenging."
      - Ensure that at least two distinct label columns are created.
   7. Apply both cutting-edge NLP techniques (e.g., transformers) and legacy techniques.

### 2. Modeling
   - Provide results for the top three algorithms (e.g., classifiers) used. You are encouraged to use:
      - Stacking.
      - Deep learning algorithms.
      - Transformers.
   - Show results from each algorithm based on parameter tuning.

### 3. Evaluation
   1. Recommend the best metric for model performance evaluation.
   2. Provide at least the following three metrics for each result:
      - Accuracy.
      - Recall.
      - Precision.

## Deliverables:
After successfully implementing the above requirements, you are to submit the following:

1. A report documenting your implementation of the above requirements.
2. The source code (preferably written in Python, .ipynb) with proper documentation.

### Additional Credit:
Candidates who use modern software development tools (e.g., integrated development environments, automation scripts) and version control systems (e.g., Git) will receive extra credit. Leveraging the latest NLP techniques (e.g., transfer learning, pre-trained language models) is strongly encouraged.
"""

!pip install -q -U Arabic-Stopwords transformers Tashaphyne emoji openpyxl googletrans deep_translator
!pip install -q -U unidecode emoji aiogoogletrans torchmetrics pytorch_lightning



!python -m textblob.download_corpora
!git clone https://github.com/saobou/DSAraby.git

# Commented out IPython magic to ensure Python compatibility.
# Add environment Packages paths to conda
import os, sys, warnings
import nltk
import string
import re
import regex
import emoji
from nltk.corpus import stopwords # Stopwords
import arabicstopwords.arabicstopwords as stp #more range of arabic stop words
from nltk.stem.isri import ISRIStemmer
import pyarabic.araby as araby
from tashaphyne.stemming import ArabicLightStemmer
from nltk.stem import SnowballStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split,KFold, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix, classification_report
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score
from sklearn.svm import SVC
import pickle

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
from pylab import rcParams
from matplotlib import rc
import joblib

from transformers import AutoTokenizer, AutoModel
import torch
from torch import nn,optim
from torch.utils.data import Dataset, DataLoader
from torchmetrics import F1Score
import pytorch_lightning as pl
from pytorch_lightning.callbacks.early_stopping import EarlyStopping

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report

from tqdm.auto import tqdm
warnings.simplefilter("ignore")

# Visualization Packages
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(font_scale=1.3)
# %matplotlib inline

#are there any stop words in the data?
nltk.download('stopwords')
arabic_stopwords = stopwords.words("arabic")
len(arabic_stopwords)





import pandas as pd
df = pd.read_excel("/content/drive/MyDrive/Sigma_Task/Book1.xlsx")
df.head()

df_process = df.copy()

df_process.drop(columns=["No.", "Date", "1st-Response Date", "Close Date", "Device Serial Number", "Brand Name"], inplace=True)

df_process.info()

df_process.dropna(inplace=True)
df_process.info()



import emoji
#Stats about Text
def avg_word(sentence):
    words = sentence.split()
    if len(words) == 0:
        return 0
    return (sum(len(word) for word in words)/len(words))

def emoji_counter(sentence):
    return emoji.emoji_count(sentence)

df_process['word_count'] = df_process['Request Detail'].apply(lambda x: len(str(x).split(" ")))
df_process['char_count'] = df_process['Request Detail'].str.len()
df_process['avg_char_per_word'] = df_process['Request Detail'].apply(lambda x: avg_word(x))
stop = stopwords.words('arabic')
df_process['stopwords'] = df_process['Request Detail'].apply(lambda x: len([x for x in x.split() if x in stop]))
df_process['emoji_count'] = df_process['Request Detail'].apply(lambda x: emoji_counter(x))
train = df_process.sort_values(by='word_count',ascending=[0])



# @title Status vs word_count

from matplotlib import pyplot as plt
import seaborn as sns
figsize = (12, 1.2 * len(df_process['Status'].unique()))
plt.figure(figsize=figsize)
sns.violinplot(df_process, x='word_count', y='Status', inner='box', palette='Dark2')
sns.despine(top=True, right=True, bottom=True, left=True)

# import emoji
def avg_word(sentence):
    words = str(sentence).split()
    if len(words) == 0:
        return 0
    return (sum(len(word) for word in words)/len(words))

def emoji_counter(sentence):
    # Convert sentence to string to handle potential floats
    return emoji.emoji_count(str(sentence))

df_process['word_count_n'] = df_process['Notes'].apply(lambda x: len(str(x).split(" ")))
df_process['char_count_n'] = df_process['Notes'].str.len() ## this also includes spaces
df_process['avg_char_per_word_n'] = df_process['Notes'].apply(lambda x: avg_word(x))
stop = stopwords.words('arabic')
df_process['stopwords_n'] = df_process['Notes'].apply(lambda x: len([x for x in str(x).split() if x in stop]))
df_process['emoji_count_n'] = df_process['Notes'].apply(lambda x: emoji_counter(x)) # Changed line
train = df_process.sort_values(by='Notes',ascending=[0])
df_process.head()

"""# Data cleaning (e.g. remove punctuation, stemming, stopwords, etc.)"""

# !pip install PyArabic
import pyarabic.araby as araby
import pyarabic.number as number
from pyarabic.araby import strip_tashkeel

def transliterate(text):
  """
  This is a very basic example and needs to be expanded based on your needs.
  It only handles a few common mappings.
  """
  mapping = {
      'a': 'ا',
      'b': 'ب',
      'c': 'ك',
      'd': 'د',
      'e': 'ي',
      'f': 'ف',
      'g': 'ج',
      'h': 'ه',
      'i': 'ي',
      'j': 'ج',
      'k': 'ك',
      'l': 'ل',
      'm': 'م',
      'n': 'ن',
      'o': 'و',
      'p': 'ب',
      'q': 'ق',
      'r': 'ر',
      's': 'س',
      't': 'ت',
      'u': 'و',
      'v': 'ف',
      'w': 'و',
      'x': 'كس',
      'y': 'ي',
      'z': 'ز',
      ' ': ' ',
  }
  transliterated_text = ''.join([mapping.get(char, char) for char in text.lower()])
  return strip_tashkeel(transliterated_text)

from nltk.corpus import stopwords
from textblob import TextBlob
import re
from tashaphyne.stemming import ArabicLightStemmer
from nltk.stem.isri import ISRIStemmer

stops = set(stopwords.words("arabic"))
stop_word_comp = {"،","آض","آمينَ","آه","آهاً","آي","أ","أب","أجل","أجمع","أخ","أخذ","أصبح","أضحى","أقبل","أقل","أكثر","ألا","أم","أما","أمامك","أمامكَ","أمسى","أمّا","أن","أنا","أنت","أنتم","أنتما","أنتن","أنتِ","أنشأ","أنّى","أو","أوشك","أولئك","أولئكم","أولاء","أولالك","أوّهْ","أي","أيا","أين","أينما","أيّ","أَنَّ","أََيُّ","أُفٍّ","إذ","إذا","إذاً","إذما","إذن","إلى","إليكم","إليكما","إليكنّ","إليكَ","إلَيْكَ","إلّا","إمّا","إن","إنّما","إي","إياك","إياكم","إياكما","إياكن","إيانا","إياه","إياها","إياهم","إياهما","إياهن","إياي","إيهٍ","إِنَّ","ا","ابتدأ","اثر","اجل","احد","اخرى","اخلولق","اذا","اربعة","ارتدّ","استحال","اطار","اعادة","اعلنت","اف","اكثر","اكد","الألاء","الألى","الا","الاخيرة","الان","الاول","الاولى","التى","التي","الثاني","الثانية","الذاتي","الذى","الذي","الذين","السابق","الف","اللائي","اللاتي","اللتان","اللتيا","اللتين","اللذان","اللذين","اللواتي","الماضي","المقبل","الوقت","الى","اليوم","اما","امام","امس","ان","انبرى","انقلب","انه","انها","او","اول","اي","ايار","ايام","ايضا","ب","بات","باسم","بان","بخٍ","برس","بسبب","بسّ","بشكل","بضع","بطآن","بعد","بعض","بك","بكم","بكما","بكن","بل","بلى","بما","بماذا","بمن","بن","بنا","به","بها","بي","بيد","بين","بَسْ","بَلْهَ","بِئْسَ","تانِ","تانِك","تبدّل","تجاه","تحوّل","تلقاء","تلك","تلكم","تلكما","تم","تينك","تَيْنِ","تِه","تِي","ثلاثة","ثم","ثمّ","ثمّة","ثُمَّ","جعل","جلل","جميع","جير","حار","حاشا","حاليا","حاي","حتى","حرى","حسب","حم","حوالى","حول","حيث","حيثما","حين","حيَّ","حَبَّذَا","حَتَّى","حَذارِ","خلا","خلال","دون","دونك","ذا","ذات","ذاك","ذانك","ذانِ","ذلك","ذلكم","ذلكما","ذلكن","ذو","ذوا","ذواتا","ذواتي","ذيت","ذينك","ذَيْنِ","ذِه","ذِي","راح","رجع","رويدك","ريث","رُبَّ","زيارة","سبحان","سرعان","سنة","سنوات","سوف","سوى","سَاءَ","سَاءَمَا","شبه","شخصا","شرع","شَتَّانَ","صار","صباح","صفر","صهٍ","صهْ","ضد","ضمن","طاق","طالما","طفق","طَق","ظلّ","عاد","عام","عاما","عامة","عدا","عدة","عدد","عدم","عسى","عشر","عشرة","علق","على","عليك","عليه","عليها","علًّ","عن","عند","عندما","عوض","عين","عَدَسْ","عَمَّا","غدا","غير","ـ","ف","فان","فلان","فو","فى","في","فيم","فيما","فيه","فيها","قال","قام","قبل","قد","قطّ","قلما","قوة","كأنّما","كأين","كأيّ","كأيّن","كاد","كان","كانت","كذا","كذلك","كرب","كل","كلا","كلاهما","كلتا","كلم","كليكما","كليهما","كلّما","كلَّا","كم","كما","كي","كيت","كيف","كيفما","كَأَنَّ","كِخ","لئن","لا","لات","لاسيما","لدن","لدى","لعمر","لقاء","لك","لكم","لكما","لكن","لكنَّما","لكي","لكيلا","للامم","لم","لما","لمّا","لن","لنا","له","لها","لو","لوكالة","لولا","لوما","لي","لَسْتَ","لَسْتُ","لَسْتُم","لَسْتُمَا","لَسْتُنَّ","لَسْتِ","لَسْنَ","لَعَلَّ","لَكِنَّ","لَيْتَ","لَيْسَ","لَيْسَا","لَيْسَتَا","لَيْسَتْ","لَيْسُوا","لَِسْنَا","ما","ماانفك","مابرح","مادام","ماذا","مازال","مافتئ","مايو","متى","مثل","مذ","مساء","مع","معاذ","مقابل","مكانكم","مكانكما","مكانكنّ","مكانَك","مليار","مليون","مما","ممن","من","منذ","منها","مه","مهما","مَنْ","مِن","نحن","نحو","نعم","نفس","نفسه","نهاية","نَخْ","نِعِمّا","نِعْمَ","ها","هاؤم","هاكَ","هاهنا","هبّ","هذا","هذه","هكذا","هل","هلمَّ","هلّا","هم","هما","هن","هنا","هناك","هنالك","هو","هي","هيا","هيت","هيّا","هَؤلاء","هَاتانِ","هَاتَيْنِ","هَاتِه","هَاتِي","هَجْ","هَذا","هَذانِ","هَذَيْنِ","هَذِه","هَذِي","هَيْهَاتَ","و","و6","وا","واحد","واضاف","واضافت","واكد","وان","واهاً","واوضح","وراءَك","وفي","وقال","وقالت","وقد","وقف","وكان","وكانت","ولا","ولم","ومن","مَن","وهو","وهي","ويكأنّ","وَيْ","وُشْكَانََ","يكون","يمكن","يوم","ّأيّان"}
ArListem = ArabicLightStemmer()


# def to_arabic(text):
#     return transliterate(text)

def stem(text):
    zen = TextBlob(text)
    words = zen.words
    cleaned = list()
    for w in words:
        ArListem.light_stem(w)
        cleaned.append(ArListem.get_root())
    return " ".join(cleaned)

import pyarabic.araby as araby
def normalizeArabic(text):
    text = text.strip()
    text = re.sub("[إأٱآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "ء", text)
    text = re.sub("ئ", "ء", text)
    text = re.sub("ة", "ه", text)
    noise = re.compile(""" ّ    | # Tashdid
                             َ    | # Fatha
                             ً    | # Tanwin Fath
                             ُ    | # Damma
                             ٌ    | # Tanwin Damm
                             ِ    | # Kasra
                             ٍ    | # Tanwin Kasr
                             ْ    | # Sukun
                             ـ     # Tatwil/Kashida
                         """, re.VERBOSE)
    text = re.sub(noise, '', text)
    text = re.sub(r'(.)\1+', r"\1\1", text) # Remove longation
    return araby.strip_tashkeel(text)

def remove_stop_words(text):
    zen = TextBlob(text)
    words = zen.words
    return " ".join([w for w in words if not w in stops and not w in stop_word_comp and len(w) >= 2])

def split_hashtag_to_words(tag):
    tag = tag.replace('#','')
    tags = tag.split('_')
    if len(tags) > 1 :
        return tags
    pattern = re.compile(r"[A-Z][a-z]+|\d+|[A-Z]+(?![a-z])")
    return pattern.findall(tag)

def clean_hashtag(text):
    words = text.split()
    text = list()
    for word in words:
        if is_hashtag(word):
            text.extend(extract_hashtag(word))
        else:
            text.append(word)
    return " ".join(text)
def is_hashtag(word):
    if word.startswith("#"):
        return True
    else:
        return False
def extract_hashtag(text):

    hash_list = ([re.sub(r"(\W+)$", "", i) for i in text.split() if i.startswith("#")])
    word_list = []
    for word in hash_list :
        word_list.extend(split_hashtag_to_words(word))
    return word_list

from __future__ import unicode_literals
from aiogoogletrans import Translator
translator = Translator()
import asyncio
loop = asyncio.get_event_loop()
import unicodedata
from unidecode import unidecode

def remove_emoji(text):
    emoji_pattern = re.compile("["
                                   u"\U0001F600-\U0001F64F"  # emoticons
                                   u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                                   u"\U0001F680-\U0001F6FF"  # transport & map symbols
                                   u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                                   u"\U00002702-\U000027B0"
                                   u"\U000024C2-\U0001F251"
                                   "]+", flags=re.UNICODE)
    text = emoji_pattern.sub(r'', text)
    return text


def emoji_native_translation(text):
    text = text.lower()
    loves = ["<3", "♥",'❤']
    smilefaces = []
    sadfaces = []
    neutralfaces = []
    eyes = ["8",":","=",";"]
    nose = ["'","`","-",r"\\"]
    for e in eyes:
        for n in nose:
            for s in ["\)", "d", "]", "}","p"]:
                smilefaces.append(e+n+s)
                smilefaces.append(e+s)
            for s in ["\(", "\[", "{"]:
                sadfaces.append(e+n+s)
                sadfaces.append(e+s)
            for s in ["\|", "\/", r"\\"]:
                neutralfaces.append(e+n+s)
                neutralfaces.append(e+s)
            for s in ["\(", "\[", "{"]:
                smilefaces.append(s+n+e)
                smilefaces.append(s+e)
            for s in ["\)", "\]", "}"]:
                sadfaces.append(s+n+e)
                sadfaces.append(s+e)
            for s in ["\|", "\/", r"\\"]:
                neutralfaces.append(s+n+e)
                neutralfaces.append(s+e)

    smilefaces = list(set(smilefaces))
    sadfaces = list(set(sadfaces))
    neutralfaces = list(set(neutralfaces))
    t = []
    for w in text.split():
        if w in loves:
            t.append("حب")
        elif w in smilefaces:
            t.append("مضحك")
        elif w in neutralfaces:
            t.append("عادي")
        elif w in sadfaces:
            t.append("محزن")
        else:
            t.append(w)
    newText = " ".join(t)
    return newText



def clean_emoji(text):
    text = emoji_native_translation(text)
    return text
def clean_tweet(text):
    text = re.sub('#\d+K\d+', ' ', text)  # years like 2K19
    text = re.sub('http\S+\s*', ' ', text)  # remove URLs
    text = re.sub('RT|cc', ' ', text)  # remove RT and cc
    text = re.sub('@[^\s]+',' ',text)
    text = clean_hashtag(text)
    text = clean_emoji(text)
    return text

def clean_text(text):
    ## Clean for tweets
    text = clean_tweet(text)
    ## Remove punctuations
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,،-./:;<=>؟?@[\]^_`{|}~"""), ' ', text)  # remove punctuation
    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    ## Remove Emojis
    text = remove_emoji(text)
    ## Convert text to lowercases
    text = text.lower()
    ## Arabisy the text
    # text = to_arabic(text)
    ## Remove stop words
    text = remove_stop_words(text)
    ## Remove numbers
    text = re.sub("\d+", " ", text)
    ## Remove Tashkeel
    text = normalizeArabic(text)
    #text = re.sub('\W+', ' ', text)
    text = re.sub('[A-Za-z]+',' ',text)
    text = re.sub(r'\\u[A-Za-z0-9\\]+',' ',text)
    ## remove extra whitespace
    text = re.sub('\s+', ' ', text)
    #Stemming
    #text = stem(text)
    return text

df_process['Request Detail'] = df_process['Request Detail'].apply(lambda x:clean_text(x))
df_process["Request Detail"][0]

df["Request Detail"][0]

import re
from deep_translator import GoogleTranslator  # use deep_translator
translator = GoogleTranslator(source='en', target='ar')

def clean_text(text):
    ## Clean for tweets
    text = clean_tweet(text)
    ## Remove punctuations
    text = re.sub('[%s]' % re.escape("""!"#$%&'()*+,،-./:;<=>؟?@[\]^_`{|}~"""), ' ', text)  # remove punctuation
    ## Remove extra whitespace
    text = re.sub('\s+', ' ', text)
    ## Remove Emojis
    text = remove_emoji(text)
    ## Convert text to lowercase
    text = text.lower()
    ## Remove stop words
    text = remove_stop_words(text)
    ## Remove numbers
    text = re.sub("\d+", " ", text)
    ## Remove Tashkeel (diacritics)
    text = normalizeArabic(text)
    # Check if the text exceeds the character limit
    if len(text) < 5000:
        # Translate from English to Arabic
        text = translator.translate(text)
    else:
        # Handle text exceeding the limit (e.g., split and translate in chunks)
        text_chunks = [text[i:i+4500] for i in range(0, len(text), 4500)]
        translated_chunks = [translator.translate(chunk) for chunk in text_chunks]
        text = ' '.join(translated_chunks)
    ## Remove any remaining non-Arabic or unwanted characters
    text = re.sub('[A-Za-z]+',' ',text)
    text = re.sub(r'\\u[A-Za-z0-9\\]+',' ',text)
    ## Remove extra whitespace
    text = re.sub('\s+', ' ', text)
    return text


df_process['Notes'] = df_process['Notes'].apply(lambda x:clean_text(x))
df_process["Notes"][500]

# df_process['Notes'] = df_process['Notes'].apply(lambda x:clean_text(x))
# df_process = df_process[df_process['Notes'] != ' ']
# df_process.dropna(inplace=True)
# df_process["Notes"][500]

df["Notes"][500]

df_process.head()

import plotly.express as px
fig = px.bar(df_process["Subject"].value_counts().reset_index(), x="Subject", y="count", title="Subject Value Counts") # The x value should correspond to the column containing the unique values from the Subject column. The y value should correspond to the counts.
fig.show()

df_process["Subject"].value_counts()

df_process.info()

df_process.to_excel("df_prcess.xlsx")

df_process.isnull().sum()

df_sample = df_process.copy()
df_sample['text'] = df_sample['Request Detail'] + ' ' + df_sample['Notes']
df_sample = df_sample[['text']]
df_sample.to_excel("requet_notesT.xlsx")

df_sample.head()

df_process.info()

"""# EDA"""

import plotly.graph_objects as go
import plotly.express as px

# Chart 1: Distribution of Request Types
fig1 = px.histogram(df_process, x='Request Type', title='Distribution of Request Types')
fig1.show()

# Chart 2: Status Count by Priority
fig2 = px.histogram(df_process, x='Priority', color='Status', title='Status Count by Priority')
fig2.show()

# Chart 3:  Subject Word Cloud
text = " ".join(df_process['Subject'].astype(str))
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

fig3 = px.imshow(wordcloud, title='Subject Word Cloud')
fig3.show()

# Chart 4:  Request Type by Company
# Use a bar chart to show the count or percentage of each Request Type by Company.
fig4 = px.histogram(df_process, x='Company', color='Request Type',
                   title='Request Type by Company')
fig4.show()

"""* Show words most used in “Request Detail” feature and present it in a nice way."""

from collections import Counter
import plotly.express as px

# Combine all the text from the "Request Detail" feature into a single string
all_text = ' '.join(df_process['Request Detail'].astype(str))

# Split the text into words
words = all_text.split()

# Count the frequency of each word
word_counts = Counter(words)

# Get the top N most frequent words (e.g., top 20)
top_n = 20
most_common_words = word_counts.most_common(top_n)

# Extract words and their counts for the plot
word_labels = [word for word, count in most_common_words]
word_counts_list = [count for word, count in most_common_words]


# Create a bar chart with Plotly
fig = px.bar(
    x=word_labels,
    y=word_counts_list,
    labels={'x': 'Words', 'y': 'Frequency'},
    title=f'Top {top_n} Most Frequent Words in Request Detail'
)
fig.show()

import plotly.graph_objects as go
from wordcloud import WordCloud

text = " ".join(df_process['Request Detail'].astype(str))
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

fig5 = go.Figure(go.Image(z=wordcloud))
fig5.update_layout(title='Request Detail Word Cloud')
fig5.show()

text = " ".join(df_process['Notes'].astype(str))
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

fig6 = go.Figure(go.Image(z=wordcloud))
fig6.update_layout(title='Notes Word Cloud')
fig6.show()

# prompt: c. Study correlation
# d. Show distributions and demotions of data
# e. Show statistical abilities and tendency metrics make those with plotly graph  but first conver column to a better for visilization

import plotly.express as px
import plotly.figure_factory as ff
import numpy as np


# d. Show distributions and demotions of data
fig_priority_dist = px.histogram(df_process, x='Priority', title='Distribution of Priority')
fig_priority_dist.show()


# e. Show statistical abilities and tendency metrics
# Example: Distribution of 'word_count_n'
fig_word_count_dist = ff.create_distplot([df_process['word_count_n']], ['Word Count'], bin_size=0.5)
fig_word_count_dist.update_layout(title='Distribution of Word Count')
fig_word_count_dist.show()


# Example: Box plot of 'avg_char_per_word_n' by 'Status'
fig_box_avg_char = px.box(df_process, x='Status', y='avg_char_per_word_n', title='Average Character per Word by Status')
fig_box_avg_char.show()


# Convert 'Status' to numerical for better visualization (if needed)
status_mapping = {status: i for i, status in enumerate(df_process['Status'].unique())}
df_process['Status_numerical'] = df_process['Status'].map(status_mapping)

# Example: Scatter plot of 'word_count_n' vs 'Status_numerical'
fig_scatter_word_status = px.scatter(df_process, x='word_count_n', y='Status_numerical', title='Word Count vs Status (Numerical)')
fig_scatter_word_status.show()

import plotly.graph_objects as go
from collections import Counter

# Combine Request Detail and Notes for analysis
combined_text = " ".join(df_process['Request Detail'].astype(str) + " " + df_process['Notes'].astype(str))

# Tokenize the combined text
tokens = combined_text.split()

# Count the frequency of each word
word_counts = Counter(tokens)

# Identify the most common words
most_common_words = word_counts.most_common(20)

# Create a bar chart with Plotly Graph Objects
fig = go.Figure(data=[go.Bar(x=[word for word, count in most_common_words],
                             y=[count for word, count in most_common_words])])

fig.update_layout(title='Most Frequent Words in Request Detail and Notes',
                  xaxis_title='Words',
                  yaxis_title='Frequency')

fig.show()

df_process.head()

df_process["Subject"].value_counts()

import pandas as pd
import plotly.graph_objs as go
from plotly.subplots import make_subplots


# Calculate metrics for 'Request Detail'
df_process['word_count_request'] = df_process['Request Detail'].apply(lambda x: len(x.split()))
df_process['char_count_request'] = df_process['Request Detail'].apply(len)
df_process['avg_char_per_word_request'] = df_process['char_count_request'] / df_process['word_count_request']

# Calculate metrics for 'Notes'
df_process['word_count_notes'] = df_process['Notes'].apply(lambda x: len(x.split()))
df_process['char_count_notes'] = df_process['Notes'].apply(len)
df_process['avg_char_per_word_notes'] = df_process['char_count_notes'] / df_process['word_count_notes']

# Create a subplot figure with 4 rows and 1 column
fig = make_subplots(
    rows=4, cols=1,
    subplot_titles=(
        "Word Count Comparison",
        "Character Count Comparison",
        "Avg. Characters per Word",
        "Correlation Analysis"
    ),
    vertical_spacing=0.1
)

# Word Count Comparison
trace1 = go.Box(
    y=df_process['word_count_request'],
    name='Request Detail',
    marker=dict(color='blue'),
    boxmean='sd'  # Shows mean and standard deviation
)
trace2 = go.Box(
    y=df_process['word_count_notes'],
    name='Notes',
    marker=dict(color='orange'),
    boxmean='sd'
)

# Character Count Comparison
trace3 = go.Box(
    y=df_process['char_count_request'],
    name='Request Detail',
    marker=dict(color='blue'),
    boxmean='sd'
)
trace4 = go.Box(
    y=df_process['char_count_notes'],
    name='Notes',
    marker=dict(color='orange'),
    boxmean='sd'
)

# Avg. Characters per Word
trace5 = go.Box(
    y=df_process['avg_char_per_word_request'],
    name='Request Detail',
    marker=dict(color='blue'),
    boxmean='sd'
)
trace6 = go.Box(
    y=df_process['avg_char_per_word_notes'],
    name='Notes',
    marker=dict(color='orange'),
    boxmean='sd'
)

# Correlation Analysis (Scatter Plot)
trace7 = go.Scatter(
    x=df_process['word_count_request'],
    y=df_process['word_count_notes'],
    mode='markers+lines',
    name='Word Count Correlation',
    marker=dict(color='green'),
    line=dict(dash='dot')
)
trace8 = go.Scatter(
    x=df_process['char_count_request'],
    y=df_process['char_count_notes'],
    mode='markers+lines',
    name='Character Count Correlation',
    marker=dict(color='purple'),
    line=dict(dash='dot')
)

# Add traces to the figure
fig.add_trace(trace1, row=1, col=1)
fig.add_trace(trace2, row=1, col=1)

fig.add_trace(trace3, row=2, col=1)
fig.add_trace(trace4, row=2, col=1)

fig.add_trace(trace5, row=3, col=1)
fig.add_trace(trace6, row=3, col=1)

fig.add_trace(trace7, row=4, col=1)
fig.add_trace(trace8, row=4, col=1)

# Update layout
fig.update_layout(
    height=1000,  # Adjust height to fit multiple rows
    title_text="Discrepancies and Correlations between 'Request Detail' and 'Notes' Features in Arabic",
    showlegend=False,  # Turn off legend for clarity
    template='plotly_white'
)

# Show the plot
fig.show()

from sklearn.feature_extraction.text import TfidfVectorizer
import plotly.express as px
import plotly.graph_objects as go


# 2. Feature Extraction using TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix = tfidf_vectorizer.fit_transform(df_process['Request Detail'])

# 3. Convert TF-IDF matrix to DataFrame for easier correlation calculation
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())


# 2. Feature Extraction using TF-IDF
tfidf_vectorizer_s = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix_s = tfidf_vectorizer_s.fit_transform(df_process['Subject'])

# 3. Convert TF-IDF matrix to DataFrame for easier correlation calculation
tfidf_df_s = pd.DataFrame(tfidf_matrix_s.toarray(), columns=tfidf_vectorizer_s.get_feature_names_out())


# 4. Combine TF-IDF features with 'Subject'
df_combined = pd.concat([tfidf_df_s, tfidf_df], axis=1)

# 5. Calculate Correlation Matrix
correlation_matrix = df_combined.corr()

# 6. Visualize Correlation with Plotly
fig = px.imshow(correlation_matrix,
                labels=dict(x="Features", y="Features", color="Correlation"),
                x=correlation_matrix.columns,
                y=correlation_matrix.columns,
                color_continuous_scale='RdBu',
                title="Correlation Matrix: Subject vs. Request Detail TF-IDF Features")

fig.update_layout(
    width=1200,  # Adjust width and height as needed
    height=1200,
)

fig.show()



"""# 3D Correlation between Subject and Request Detail"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import plotly.express as px

# 2. Feature Extraction using TF-IDF
tfidf_vectorizer_subject = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix_subject = tfidf_vectorizer_subject.fit_transform(df_process['Subject'])

tfidf_vectorizer_request = TfidfVectorizer(max_features=500)
tfidf_matrix_request = tfidf_vectorizer_request.fit_transform(df_process['Request Detail'])

# 3. Dimensionality Reduction using SVD (for 3D visualization)
svd = TruncatedSVD(n_components=3)  # Reduce to 3 dimensions
reduced_subject = svd.fit_transform(tfidf_matrix_subject)
reduced_request = svd.fit_transform(tfidf_matrix_request)


# 4. Create a DataFrame for visualization
df_for_visualization = pd.DataFrame({
    'Subject_Dim1': reduced_subject[:, 0],
    'Subject_Dim2': reduced_subject[:, 1],
    'Subject_Dim3': reduced_subject[:, 2],
    'Request_Dim1': reduced_request[:, 0],
    'Request_Dim2': reduced_request[:, 1],
    'Request_Dim3': reduced_request[:, 2],
    'Subject': df_process['Subject'],
    'Request Detail': df_process['Request Detail']
})


# 5. Create a 3D correlation plot
fig = px.scatter_3d(
    df_for_visualization,
    x='Subject_Dim1',
    y='Subject_Dim2',
    z='Subject_Dim3',
    color='Request_Dim1',  # You can change the color based on other variables
    hover_data=['Subject', 'Request Detail'],
    title='3D Correlation: Subject vs. Request Detail (TF-IDF with SVD)'
)

fig.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
import plotly.express as px

# Assuming df_process is your DataFrame

# 1. Process Text and Feature Extraction (TF-IDF)
tfidf_vectorizer_subject = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix_subject = tfidf_vectorizer_subject.fit_transform(df_process['Subject'])

tfidf_vectorizer_request = TfidfVectorizer(max_features=500)
tfidf_matrix_request = tfidf_vectorizer_request.fit_transform(df_process['Request Detail'])

# 2. Dimensionality Reduction using PCA (for 3D visualization)
pca = PCA(n_components=3)  # Reduce to 3 dimensions
reduced_subject = pca.fit_transform(tfidf_matrix_subject.toarray())
reduced_request = pca.fit_transform(tfidf_matrix_request.toarray())

# 3. Create a DataFrame for visualization
df_for_visualization = pd.DataFrame({
    'Subject_Dim1': reduced_subject[:, 0],
    'Subject_Dim2': reduced_subject[:, 1],
    'Subject_Dim3': reduced_subject[:, 2],
    'Request_Dim1': reduced_request[:, 0],
    'Request_Dim2': reduced_request[:, 1],
    'Request_Dim3': reduced_request[:, 2],
    'Subject': df_process['Subject'],
    'Request Detail': df_process['Request Detail']
})

# 4. Create a 3D correlation plot
fig = px.scatter_3d(
    df_for_visualization,
    x='Subject_Dim1',
    y='Subject_Dim2',
    z='Subject_Dim3',
    color='Request_Dim1',  # You can change the color based on other variables
    hover_data=['Subject', 'Request Detail'],
    title='3D Correlation: Subject vs. Request Detail (TF-IDF with PCA)'
)

fig.show()

"""# 3D Correlation between Subject and Notes"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import plotly.express as px

# 2. Feature Extraction using TF-IDF
tfidf_vectorizer_subject = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix_subject = tfidf_vectorizer_subject.fit_transform(df_process['Subject'])

tfidf_vectorizer_notes = TfidfVectorizer(max_features=500)
tfidf_matrix_notes = tfidf_vectorizer_notes.fit_transform(df_process['Notes'])

# 3. Dimensionality Reduction using SVD (for 3D visualization)
svd = TruncatedSVD(n_components=3)  # Reduce to 3 dimensions
reduced_subject = svd.fit_transform(tfidf_matrix_subject)
reduced_notes = svd.fit_transform(tfidf_matrix_notes)


# 4. Create a DataFrame for visualization
df_for_visualization = pd.DataFrame({
    'Subject_Dim1': reduced_subject[:, 0],
    'Subject_Dim2': reduced_subject[:, 1],
    'Subject_Dim3': reduced_subject[:, 2],
    'Notes_Dim1': reduced_notes[:, 0],
    'Notes_Dim2': reduced_notes[:, 1],
    'Notes_Dim3': reduced_notes[:, 2],
    'Subject': df_process['Subject'],
    'Notes': df_process['Notes']
})


# 5. Create a 3D correlation plot
fig = px.scatter_3d(
    df_for_visualization,
    x='Subject_Dim1',
    y='Subject_Dim2',
    z='Subject_Dim3',
    color='Notes_Dim1',  # You can change the color based on other variables
    hover_data=['Subject', 'Notes'],
    title='3D Correlation: Subject vs. Notes (TF-IDF with SVD)')
fig.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
import plotly.express as px

# 2. Process Text and Feature Extraction (TF-IDF)
tfidf_vectorizer_subject = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix_subject = tfidf_vectorizer_subject.fit_transform(df_process['Subject'])

tfidf_vectorizer_notes = TfidfVectorizer(max_features=500)
tfidf_matrix_notes = tfidf_vectorizer_notes.fit_transform(df_process['Notes'])

# 3. Dimensionality Reduction using PCA (for 3D visualization)
pca = PCA(n_components=3)  # Reduce to 3 dimensions
reduced_subject = pca.fit_transform(tfidf_matrix_subject.toarray())
reduced_notes = pca.fit_transform(tfidf_matrix_notes.toarray())

# 4. Create a DataFrame for visualization
df_for_visualization = pd.DataFrame({
    'Subject_Dim1': reduced_subject[:, 0],
    'Subject_Dim2': reduced_subject[:, 1],
    'Subject_Dim3': reduced_subject[:, 2],
    'Notes_Dim1': reduced_notes[:, 0],
    'Notes_Dim2': reduced_notes[:, 1],
    'Notes_Dim3': reduced_notes[:, 2],
    'Subject': df_process['Subject'],
    'Notes': df_process['Notes']
})

# 5. Create a 3D correlation plot
fig = px.scatter_3d(
    df_for_visualization,
    x='Subject_Dim1',
    y='Subject_Dim2',
    z='Subject_Dim3',
    color='Notes_Dim1',  # You can change the color based on other variables
    hover_data=['Subject', 'Notes'],
    title='3D Correlation: Subject vs. Notes (TF-IDF with PCA)'
)

fig.show()

"""# 3D Correlation between Notes and Rquest Details"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
import plotly.express as px

# 2. Feature Extraction using TF-IDF
tfidf_vectorizer_request = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix_request = tfidf_vectorizer_request.fit_transform(df_process['Request Detail'])

tfidf_vectorizer_notes = TfidfVectorizer(max_features=500)
tfidf_matrix_notes = tfidf_vectorizer_notes.fit_transform(df_process['Notes'])

# 3. Dimensionality Reduction using SVD (for 3D visualization)
svd = TruncatedSVD(n_components=3)  # Reduce to 3 dimensions
reduced_request = svd.fit_transform(tfidf_matrix_request)
reduced_notes = svd.fit_transform(tfidf_matrix_notes)


# 4. Create a DataFrame for visualization
df_for_visualization = pd.DataFrame({
    'Request_Dim1': reduced_request[:, 0],
    'Request_Dim2': reduced_request[:, 1],
    'Request_Dim3': reduced_request[:, 2],
    'Notes_Dim1': reduced_notes[:, 0],
    'Notes_Dim2': reduced_notes[:, 1],
    'Notes_Dim3': reduced_notes[:, 2],
    'Request Detail': df_process['Request Detail'],
    'Notes': df_process['Notes']
})


# 5. Create a 3D correlation plot
fig = px.scatter_3d(
    df_for_visualization,
    x='Request_Dim1',
    y='Request_Dim2',
    z='Request_Dim3',
    color='Notes_Dim1',  # You can change the color based on other variables
    hover_data=['Request Detail', 'Notes'],
    title='3D Correlation: Request Detail vs. Notes (TF-IDF with SVD)'
)

fig.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
import plotly.express as px


# 1. Process Text and Feature Extraction (TF-IDF)
tfidf_vectorizer_request = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix_request = tfidf_vectorizer_request.fit_transform(df_process['Request Detail'])

tfidf_vectorizer_notes = TfidfVectorizer(max_features=500)
tfidf_matrix_notes = tfidf_vectorizer_notes.fit_transform(df_process['Notes'])

# 2. Dimensionality Reduction using PCA (for 3D visualization)
pca = PCA(n_components=3)  # Reduce to 3 dimensions
reduced_request = pca.fit_transform(tfidf_matrix_request.toarray())
reduced_notes = pca.fit_transform(tfidf_matrix_notes.toarray())

# 3. Create a DataFrame for visualization
df_for_visualization = pd.DataFrame({
    'Request_Dim1': reduced_request[:, 0],
    'Request_Dim2': reduced_request[:, 1],
    'Request_Dim3': reduced_request[:, 2],
    'Notes_Dim1': reduced_notes[:, 0],
    'Notes_Dim2': reduced_notes[:, 1],
    'Notes_Dim3': reduced_notes[:, 2],
    'Request Detail': df_process['Request Detail'],
    'Notes': df_process['Notes']})

# 4. Create a 3D correlation plot
fig = px.scatter_3d(
    df_for_visualization,
    x='Request_Dim1',
    y='Request_Dim2',
    z='Request_Dim3',
    color='Notes_Dim1',  # You can change the color based on other variables
    hover_data=['Request Detail', 'Notes'],
    title='3D Correlation: Request Detail vs. Notes (TF-IDF with PCA)'
)

fig.show()



from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
import plotly.express as px
import pandas as pd


# 1. Process Text and Feature Extraction (TF-IDF)
tfidf_vectorizer_subject = TfidfVectorizer(max_features=500)  # Adjust max_features as needed
tfidf_matrix_subject = tfidf_vectorizer_subject.fit_transform(df_process['Subject'])

tfidf_vectorizer_request = TfidfVectorizer(max_features=500)
tfidf_matrix_request = tfidf_vectorizer_request.fit_transform(df_process['Request Detail'])

tfidf_vectorizer_notes = TfidfVectorizer(max_features=500)
tfidf_matrix_notes = tfidf_vectorizer_notes.fit_transform(df_process['Notes'])

# 2. Combine TF-IDF matrices
combined_tfidf_matrix = pd.concat([
    pd.DataFrame(tfidf_matrix_subject.toarray()),
    pd.DataFrame(tfidf_matrix_request.toarray()),
    pd.DataFrame(tfidf_matrix_notes.toarray())
], axis=1)

# 3. Dimensionality Reduction using PCA (for 3D visualization)
pca = PCA(n_components=3)  # Reduce to 3 dimensions
reduced_data = pca.fit_transform(combined_tfidf_matrix)

# 4. Create a DataFrame for visualization
df_for_visualization = pd.DataFrame({
    'Dim1': reduced_data[:, 0],
    'Dim2': reduced_data[:, 1],
    'Dim3': reduced_data[:, 2],
    'Subject': df_process['Subject'],
    'Request Detail': df_process['Request Detail'],
    'Notes': df_process['Notes']
})


# 5. Create a 3D correlation plot
fig = px.scatter_3d(
    df_for_visualization,
    x='Dim1',
    y='Dim2',
    z='Dim3',
    color='Dim1',  # You can change the color based on other variables
    hover_data=['Subject', 'Request Detail', 'Notes'],
    title='3D Correlation: Subject, Request Detail, and Notes (TF-IDF with PCA)'
)
fig.show()

"""This 3D scatter plot visualizes the correlation between the 'Subject', 'Request Detail', and 'Notes' features in the dataset after applying TF-IDF and dimensionality reduction with PCA.

**Features:**

- **Subject:** The subject of the request, which provides a brief overview of the issue.
- **Request Detail:** A detailed description of the request, often providing more context.
- **Notes:** Additional notes or comments related to the request.

**Visualization:**

- Each point in the plot represents a data point (row) in the original dataset.
- The x, y, and z axes represent the principal components obtained through PCA.
- The color of each point indicates the value of the first principal component (Dim1).
- Hovering over a point provides details about the corresponding 'Subject', 'Request Detail', and 'Notes' for better understanding.

**Interpretation:**

- Points that are close together in the 3D space tend to have similar 'Subject', 'Request Detail', and 'Notes' content.
- The spatial distribution of points reveals potential relationships and patterns among the features.
- By examining the clustering of points and the color variations, you can infer how the three features are related and identify groups of requests with similar characteristics.
- This 3D visualization provides a comprehensive view of the textual data in the dataset, aiding in the exploration of intricate relationships between the three text features.
"""

subject_counts = df_process['Subject'].value_counts()
fig = go.Figure(data=[go.Bar(x=subject_counts.index, y=subject_counts.values)])
fig.update_layout(
    title="Value Counts of Subject",
    xaxis_title="Subject",
    yaxis_title="Count")

# Show the chart
fig.show()

"""The bar chart above illustrates the frequency distribution of different subjects within the dataset.

Each bar represents a unique subject, and its height corresponds to the number of times that subject appears in the data.

**Key Insights:**

* **Prevalence of Subjects:** The chart effectively visualizes which subjects are more prevalent or frequent in the dataset. Subjects with taller bars appear more frequently, indicating potential biases or a specific focus on those topics.
* **Identifying Dominant Themes:** Subjects with shorter bars occur less frequently, highlighting less dominant or less frequent themes within the data.

As final results we can see that the number of things related to (صيانه
, scan....) has a high number of occuracne

# Labeling Process

## Topic Modeling using Sentence Embeddings with concate text
"""

from sklearn.cluster import KMeans
from transformers import AutoTokenizer, AutoModel
import torch
import numpy as np


# Load the pre-trained model and tokenizer for sentence embeddings (e.g., 'aubmindlab/bert-base-arabertv02')
model_name = "aubmindlab/bert-base-arabertv02"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)


# Tokenize and encode texts to get embeddings
# Convert the Pandas Series to a list of strings
inputs = tokenizer(df_sample["text"].tolist(), padding=True, truncation=True, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()

# Apply K-Means clustering
num_topics = 3
kmeans = KMeans(n_clusters=num_topics, random_state=42)
clusters = kmeans.fit_predict(embeddings)

# Display topics
for i, text in enumerate(df["text"]):
    print(f"Text: {df["text"]}")
    print(f"Assigned Topic: {clusters[i]}")
    print("-" * 50)

"""## Zero Shot with concate text"""

from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Load a zero-shot classification pipeline
zero_shot_classifier = pipeline("zero-shot-classification", model="joeddav/xlm-roberta-large-xnli")

# Define candidate labels for zero-shot classification
candidate_labels = ["complaint", "request", "technical issue", "maintenance", "greeting", "notification"]

# Apply zero-shot classification to a few sample texts to get an idea of the labels
sample_texts = df_sample['text'].sample(5).tolist()
zero_shot_results = [zero_shot_classifier(text, candidate_labels) for text in sample_texts]

# Prepare the text data for topic modeling by vectorizing the text
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
X = vectorizer.fit_transform(df_sample['text'].astype(str))

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)
pca_result = pca.fit_transform(X.toarray())

# Display zero-shot classification results for the samples
zero_shot_results, pca_result[:5]

"""## LDA with with concate text"""

from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

# Prepare the text data for LDA by vectorizing the text
count_vectorizer = CountVectorizer(max_features=1000, stop_words='english')
X_count = count_vectorizer.fit_transform(df_sample['text'].astype(str))

# Apply LDA for topic modeling
lda = LatentDirichletAllocation(n_components=5, random_state=42)
lda_result = lda.fit_transform(X_count)

# Get the top words for each topic
def get_top_words(model, feature_names, n_top_words):
    topics = {}
    for topic_idx, topic in enumerate(model.components_):
        top_words = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]
        topics[f'Topic {topic_idx}'] = top_words
    return topics

# Extract the top words for each topic
n_top_words = 10
feature_names = count_vectorizer.get_feature_names_out()
topics = get_top_words(lda, feature_names, n_top_words)

# Display the topics
topics

"""## Labeling based on Request Detail to 8 classes"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

# 1. Vectorize the "Subject" column using TF-IDF
vectorizer = TfidfVectorizer(max_features=100)  # Limit to top 100 features for simplicity
X = vectorizer.fit_transform(df_process['Request Detail'])

# 2. Apply Dimensionality Reduction (PCA or t-SNE)
pca = PCA(n_components=2)  # Reduce to 2D for visualization
X_reduced = pca.fit_transform(X.toarray())

# 3. Optionally, apply K-Means clustering
n_clusters = 8 # You can change this to 7 or 8 based on how many categories you want
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
df_process['Cluster'] = kmeans.fit_predict(X_reduced)

# 4. Visualize the results
plt.figure(figsize=(8, 6))
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=df_process['Cluster'], cmap='viridis')
plt.colorbar()
plt.title("2D PCA of Subjects with KMeans Clustering")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

# Optional: Assign cluster labels as categories (can be done based on inspection of clusters)
df_process['Category'] = df_process['Cluster'].map({
    0: 'Hardware Issues',
    1: 'Network and Internet Issues',
    2: 'System and Software Issues',
    3: 'Peripheral Issues',
    4: 'Urgent and Critical Problems',
    5: 'Power Supply and UPS',
    6: 'Miscellaneous / Other Issues',
    7: 'Software Issues',
})

# Preview the result
print(df_process[['Request Detail', 'Category', 'Cluster']])

"""## Labeling based on Combines text of (Request Detail and Notes) to 5 classes"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Step 2: Combine 'Notes' and 'Request Detail' into one text column
df_process['combined_text'] = df_process['Notes'].fillna('') + " " + df['Request Detail'].fillna('')

# Step 3: Vectorize the text using TF-IDF
vectorizer = TfidfVectorizer(max_features=500)
X_tfidf = vectorizer.fit_transform(df_process['combined_text'])

# Step 4: Apply PCA to reduce dimensionality
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_tfidf.toarray())

# Step 5: Determine the optimal number of clusters using KMeans and elbow method
inertia = []
k_values = range(2, 20)  # Trying between 2 and 15 clusters

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_pca)
    inertia.append(kmeans.inertia_)

# Plot the elbow graph to visualize the best number of clusters
plt.figure(figsize=(8, 6))
plt.plot(k_values, inertia, marker='o')
plt.title('Elbow Method to Determine Optimal Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.grid(True)
plt.show()

# Step 6: Apply KMeans with the optimal number of clusters (assuming 5 here)
optimal_k = 5
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
df['label'] = kmeans.fit_predict(X_pca)

# Step 7: Create a dictionary to map the labels to class names
label_mapping = {
    0: 'Hardware Issue',
    1: 'Network Problem',
    2: 'Software Problem',
    3: 'Power Supply Issue',
    4: 'Peripheral Issue'}

# Step 8: Map the label column using the dictionary
df_process['class'] = df_process['label'].map(label_mapping)

# Show the first few rows with the new labels and classes
print(df_process[['Notes', 'Request Detail', 'label', 'class']].head())

df_process["class"].value_counts()

df["class"][0]

df_process["Request Detail"][0]

!pip install torchmetrics pytorch_lightning

"""# Labeling using  T5"""

# from transformers import T5Tokenizer, T5ForConditionalGeneration
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base")

# Zero-Shot Approach
# Directly asking the model to label the text.
def generate_text_labels(text, categories):
    labels = []
    text_label_mapping = {}

    # String of categories in which you want to classify the text.
    category_str = ", ".join(map(str, categories))

    # Example: I am happy today; Classify this sentence as Positive, Negative or Neutral in one word.
    for i in range(len(text)):
        input_text = f"{text[i]}; Classify this sentence as {category_str} in one word."
        input_ids = tokenizer(input_text, return_tensors="pt").input_ids

        outputs = model.generate(input_ids)
        label = tokenizer.decode(outputs[0])
        labels.append(label)
        text_label_mapping[text[i]] = label
    return labels, text_label_mapping

# Few-Shot Learning
# context = [("Sentence", "Category/Sentiment")]
def generate_text_labels_context(text, context):
    labels = []
    text_label_mapping = {}

    # Examples to help model understand the task and context
    context_string = str()
    for i in range(len(text)):
        # context_string += f"Text: {context[i][0]}\nSentiment: {context[i][1]}\n"
        context_string += f"Text: {context[i][0]}\nCategory: {context[i][1]}\n"
    for i in range(len(text)):
        input_text = f"{context_string} \nBased on the above examples determine the sentiment of the following sentence. \nText: {text[i]} \nSentiment:"
        # input_text = f"{context_string} \nBased on the above examples determine the category of the following sentence. \nText: {text[i]} \nCategory:"
        input_ids = tokenizer(input_text, return_tensors="pt").input_ids
        outputs = model.generate(input_ids)
        label = tokenizer.decode(outputs[0])
        labels.append(label)
        text_label_mapping[text[i]] = label

    return labels, text_label_mapping

df_sample = pd.read_excel("/content/requet_notesT.xlsx")

df_process.head()

candidate_labels = ["شكوى", "طلب", "مشكلة تقنية", "صيانة", "تحية", "إخطار", "اخر"]
# candidate_labels = ["complaint", "request", "technical issue", "maintenance", "greeting", "notification", "other"]
labels, text_label_mapping = generate_text_labels(df_process["Request Detail"], candidate_labels)

# Print the results
for i in range(len(df_sample["text"])):
  print("-" * 20)
  print(f"Request: {df_sample['text'][i]}")
  print(f"Predicted Label: {labels[i]}")
  print("-" * 20)

# # from transformers import T5Tokenizer, T5ForConditionalGeneration
# # Sample some requests from your dataframe (e.g., 5 samples)
# sample_requests = df_process['Request Detail'].sample(n=5).tolist()
# categories = ['Hardware Issue', 'Network Problem', 'Software Problem', 'Power Issue', 'Other']
# labels, label_mapping = generate_text_labels(sample_requests, categories)

# # Print the results
# for i in range(len(sample_requests)):
#   print(f"Request: {sample_requests[i]}")
#   print(f"Predicted Label: {labels[i]}")
#   print("-" * 20)

df_sample["class"] = labels
df_sample["class"].value_counts()



"""# Model with AraBERT"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("morit/arabic_xlm_xnli")
model = AutoModelForSequenceClassification.from_pretrained("morit/arabic_xlm_xnli")

train, val = train_test_split(df_process[['combined_text','class']], test_size=0.1, random_state=42)

train = train.rename(columns={'class':"label",'combined_text':"text"})
val = val.rename(columns={'class':"label",'combined_text':"text"})
lbl_enc = LabelEncoder()
train.loc[:,"label"] = lbl_enc.fit_transform(train["label"])
val.loc[:,"label"] = lbl_enc.transform(val["label"])
joblib.dump(lbl_enc,"label_encoder.pkl")
train.to_csv("train.csv",index=False)
val.to_csv("val.csv",index=False)

lbl_enc.classes_
{v: k for v, k in enumerate(lbl_enc.classes_)}

class ArabicDataset(Dataset):
    def __init__(self,data,max_len,model_type="Mini"):
        super().__init__()
        self.labels = data["label"].values
        self.texts = data["text"].values
        self.max_len = max_len
        model = {"Mini": "asafaya/bert-mini-arabic",
                "Medium": "asafaya/bert-medium-arabic",
                "Base": "asafaya/bert-base-arabic",
                "Large": "asafaya/bert-large-arabic"}
        self.tokenizer = AutoTokenizer.from_pretrained(model[model_type])

    def __len__(self):
        return len(self.texts)

    def __getitem__(self,idx):
        text = " ".join(self.texts[idx].split())
        label = self.labels[idx]
        inputs = self.tokenizer(text,padding='max_length',
                                max_length=self.max_len,truncation=True,return_tensors="pt")
        #input_ids,token_type_ids,attention_mask
        return {
            "inputs":{"input_ids":inputs["input_ids"][0],
                      "token_type_ids":inputs["token_type_ids"][0],
                      "attention_mask":inputs["attention_mask"][0],
                     },
            "labels": torch.tensor(label,dtype=torch.long)
        }

class ArabicDataModule(pl.LightningDataModule):
    def __init__(self,train_path,val_path,batch_size=12,max_len=100,model_type="Mini"):
        super().__init__()
        self.train_path,self.val_path= train_path,val_path
        self.batch_size = batch_size
        self.max_len = max_len
        self.model_type = model_type

    def setup(self,stage=None):
        train = pd.read_csv(self.train_path)
        val = pd.read_csv(self.val_path)
        self.train_dataset = ArabicDataset(data=train,max_len=self.max_len,model_type=self.model_type)
        self.val_dataset = ArabicDataset(data=val,max_len=self.max_len,model_type=self.model_type)

    def train_dataloader(self):
        return DataLoader(self.train_dataset,batch_size=self.batch_size,shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset,batch_size=self.batch_size,shuffle=False)

    def test_dataloader(self):
        return DataLoader(self.val_dataset,batch_size=self.batch_size,shuffle=False)

n_classes = 5
class ArabicBertModel(pl.LightningModule):
    def __init__(self,model_type="Mini"):
        super().__init__()
        model = {"Mini": ("asafaya/bert-mini-arabic",256),
                "Medium": ("asafaya/bert-medium-arabic",512),
                "Base": ("asafaya/bert-base-arabic",768),
                "Large": ("asafaya/bert-large-arabic",1024)}
        self.bert_model = AutoModel.from_pretrained(model[model_type][0])
        self.fc = nn.Linear(model[model_type][1],n_classes)

    def forward(self,inputs):
        out = self.bert_model(**inputs)#inputs["input_ids"],inputs["token_type_ids"],inputs["attention_mask"])
        last_hidden_states = out[1]
        out = self.fc(last_hidden_states)
        return out

    def configure_optimizers(self):
        return optim.AdamW(self.parameters(), lr=0.0001)

    def criterion(self,output,target):
        return nn.CrossEntropyLoss()(output,target)

    def training_step(self,batch,batch_idx):
        x,y = batch["inputs"],batch["labels"]
        out = self(x)
        loss = self.criterion(out,y)
        f1_score = F1Score(task="multiclass", num_classes=n_classes, average='macro') # Create an F1Score instance with the 'multiclass' task
        f1 = f1_score(out, y) # Calculate the F1 score by passing the predictions and labels and store it in a variable
        metrics = {"train_f1": f1, "train_loss": loss} # Log the f1 score, not the f1_score object
        self.log_dict(metrics)
        return loss

    def validation_step(self,batch,batch_idx):
        x, y = batch["inputs"],batch["labels"]
        out = self(x)
        loss = self.criterion(out,y)
        f1_score = F1Score(task="multiclass", num_classes=n_classes, average='macro') # Create an F1Score instance with the 'multiclass' task
        f1 = f1_score(out, y) # Calculate the F1 score by passing the predictions and labels and store it in a variable
        metrics = {"val_f1": f1, "val_loss": loss} # Log the f1 score, not the f1_score object
        self.log_dict(metrics)
        return metrics

# TODO: getting different models sizes results
MODEL_TYPE = "Base"
dm = ArabicDataModule(train_path="./train.csv",
                val_path = "./val.csv",
                batch_size=128, max_len=70, model_type=MODEL_TYPE)

model = ArabicBertModel(model_type=MODEL_TYPE)
trainer = pl.Trainer(max_epochs=10, default_root_dir='.') #callbacks=[EarlyStopping(monitor="val_f1")]
trainer.fit(model,dm)



torch.save(model, 'arabert_arabic_dialect.pth')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = torch.load('../input/fine-tuning-arabert/arabert_arabic_dialect.pth',  map_location=device)
model.to(device)

preds = []
real_values = []

test_dataloader = dm.test_dataloader()

progress_bar = tqdm(range(len(test_dataloader)))

model.eval()
for batch in test_dataloader:
    x,y = batch["inputs"],batch["labels"]
    inp = {k: v.to(device) for k, v in x.items()}

    with torch.no_grad():
        outputs = model(inp)

    predictions = torch.argmax(outputs, dim=1)

    preds.extend(predictions)
    real_values.extend(y)

    progress_bar.update()

preds = torch.stack(preds).cpu()
real_values = torch.stack(real_values).cpu()
print(classification_report(real_values, preds, target_names=lbl_enc.classes_))

# # prompt: apply the Roug1, Rouge2 and all types of rouge  and then BELU for the follwoing model apply it after the part of classifcation report

# # !pip install rouge-score
# from rouge_score import rouge_scorer

# # Assuming you have 'df_process' with 'Notes' and 'Request Detail' columns
# # and 'preds' containing your model's predicted labels.

# # Create a Rouge scorer
# scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

# # Iterate through your predictions and calculate ROUGE scores
# rouge_scores = []
# for i in range(len(preds)):
#   # Replace 'df_process['Notes'][i]' with the actual reference text
#   # for your predicted label.
#   reference_summary = df_process['Notes'][i]
#   # Replace 'df_process['Request Detail'][i]' with the actual generated summary for your prediction.
#   generated_summary = df_process['Request Detail'][i]

#   scores = scorer.score(reference_summary, generated_summary)
#   rouge_scores.append(scores)

# # Calculate the average ROUGE scores
# average_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])
# average_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores])
# average_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])

# print(f"Average ROUGE-1: {average_rouge1}")
# print(f"Average ROUGE-2: {average_rouge2}")
# print(f"Average ROUGE-L: {average_rougeL}")

# !pip install sacrebleu
# from sacrebleu.metrics import BLEU

# # Calculate BLEU score
# bleu_score = BLEU()
# # Assuming you have reference summaries and model generated summaries
# references = [[df_process['Notes'][i]] for i in range(len(preds))]
# hypotheses = [df_process['Request Detail'][i] for i in range(len(preds))]
# bleu = bleu_score.corpus_score(hypotheses, references)
# print(f"BLEU score: {bleu.score}")







df_unique = df_process.copy()

### Common word removal
freq = pd.Series(' '.join(df_process['Request Detail']).split()).value_counts()[:12]
freq = list(freq.index)
df_process['Request Detail'] = df_process['Request Detail'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
### Rare words removal
freq = pd.Series(' '.join(df_process['Request Detail']).split()).value_counts()[-50:]
freq = list(freq.index)
df_process['Request Detail'] = df_process['Request Detail'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))
### Discouver Data again after cleaning
df_process['word_count'] = df_process['Request Detail'].apply(lambda x: len(str(x).split(" ")))
df_process['char_count'] = df_process['Request Detail'].str.len() ## this also includes spaces
df_process['avg_char_per_word'] = df_process['Request Detail'].apply(lambda x: avg_word(x))
stop = stopwords.words('arabic')
df_process['stopwords'] = df_process['Request Detail'].apply(lambda x: len([x for x in x.split() if x in stop]))
df_process['emoji_count'] = df_process['Request Detail'].apply(lambda x: emoji_counter(x))
df_process = df_process.sort_values(by='word_count',ascending=[0])
df_process["Request Detail"][0]

### Common word removal
freq = pd.Series(' '.join(df_process['Notes'].astype(str)).split()).value_counts()[:12] # Convert the elements in the Series to string type
freq = list(freq.index)
# Convert x to string type before applying split() method
df_process['Notes'] = df_process['Notes'].apply(lambda x: " ".join(str(x) for x in str(x).split() if x not in freq))
### Rare words removal
freq = pd.Series(' '.join(df_process['Notes'].astype(str)).split()).value_counts()[-50:] # Convert the elements in the Series to string type
freq = list(freq.index)
# Convert x to string type before applying split() method
df_process['Notes'] = df_process['Notes'].apply(lambda x: " ".join(str(x) for x in str(x).split() if x not in freq))
### Discouver Data again after cleaning
df_process['word_count_n'] = df_process['Notes'].apply(lambda x: len(str(x).split(" ")))
df_process['char_count_n'] = df_process['Notes'].str.len() ## this also includes spaces
df_process['avg_char_per_word_n'] = df_process['Notes'].apply(lambda x: avg_word(x))
stop = stopwords.words('arabic')
df_process['stopwords_n'] = df_process['Notes'].apply(lambda x: len([x for x in x.split() if x in stop]))
df_process['emoji_count_n'] = df_process['Notes'].apply(lambda x: emoji_counter(x))
df_process = df_process.sort_values(by='word_count_n',ascending=[0])
df_process["Notes"][0]

df_process.to_excel("data_process.xlsx")

df_unique.head()

df_unique.to_excel("data_uniqe_ticket.xlsx")

